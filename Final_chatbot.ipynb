{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeVlDlewiKYR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np \n",
    "import re\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lK1A4mCtiKie"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class implements from Bahdanau attention in given website: (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "There are three sets of weights introduced W_a, U_a, and V_a \"\"\"\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  \n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqbD16IIiKrJ"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "lines = open('data/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('data/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2XvMHYNem_I7",
    "outputId": "1616f062-287f-4d1c-c5be-c852f6c55f9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304714"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dec7kErGm_N5",
    "outputId": "153948c8-6701-4163-b940-f5639e9bdf5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zv96qUNcm_RH"
   },
   "outputs": [],
   "source": [
    "# Creating a list of all of the conversations\n",
    "exchn = []\n",
    "for conver in conversations:\n",
    "    exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jh1vbkFUopEo"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps each line and its id\n",
    "diag = {}\n",
    "for line in lines:\n",
    "    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxc7JpFVpCra"
   },
   "outputs": [],
   "source": [
    "# Getting separately the questions and the answers\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conver in exchn:\n",
    "    for i in range(len(conver) - 1):\n",
    "        questions.append(diag[conver[i]])\n",
    "        answers.append(diag[conver[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mt3L-qeVpCuZ"
   },
   "outputs": [],
   "source": [
    "# delete\n",
    "del(lines, conversations, conver, line, diag, exchn, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4r_ZiYmpCyf"
   },
   "outputs": [],
   "source": [
    "sorted_ques = []\n",
    "sorted_ans = []\n",
    "for i in range(len(questions)):\n",
    "    if len(questions[i]) < 13:\n",
    "        sorted_ques.append(questions[i])\n",
    "        sorted_ans.append(answers[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_FYKr_lsCnI"
   },
   "outputs": [],
   "source": [
    "# Doing a first cleaning of the texts\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LTvX4cMmsCqI"
   },
   "outputs": [],
   "source": [
    "#cleaning the questions and answers\n",
    "clean_question = []\n",
    "for line in sorted_ques:\n",
    "    clean_question.append(clean_text(line))\n",
    "\n",
    "clean_answer = []        \n",
    "for line in sorted_ans:\n",
    "    clean_answer.append(clean_text(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79R_RGItsCtP"
   },
   "outputs": [],
   "source": [
    "for i in range(len(clean_answer)):\n",
    "    clean_answer[i] = ' '.join(clean_answer[i].split()[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwOli8WR_-_Y"
   },
   "outputs": [],
   "source": [
    "## delete\n",
    "del(answers, questions, line, sorted_ans, sorted_ques, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUtfmbFn__Es"
   },
   "outputs": [],
   "source": [
    "## trimming \n",
    "clean_answer=clean_answer[:30000]\n",
    "clean_question=clean_question[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tK1aPkl__IM"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps each word to its number of occurrences\n",
    "word2count = {}\n",
    "\n",
    "for line in clean_question:\n",
    "    for word in line.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "for line in clean_answer:\n",
    "    for word in line.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYvWzEMTBaEQ"
   },
   "outputs": [],
   "source": [
    "#delete\n",
    "del(word,line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozcXM5S_BgYd"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary that map the words and assign it to a unique integer that means remove less frequent\n",
    "thresh = 5\n",
    "\n",
    "vocab = {}\n",
    "word_num = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= thresh:\n",
    "        vocab[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbKZB1C_Cl4s"
   },
   "outputs": [],
   "source": [
    "## delete\n",
    "del(word2count, word, count, thresh)       \n",
    "del(word_num)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtX6Fn0_Cm2c"
   },
   "outputs": [],
   "source": [
    "# Adding the last tokens to this dictionary\n",
    "\n",
    "for i in range(len(clean_answer)):\n",
    "    clean_answer[i] = '<SOS> ' + clean_answer[i] + ' <EOS>'\n",
    "\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "x = len(vocab)\n",
    "for token in tokens:\n",
    "    vocab[token] = x\n",
    "    x += 1\n",
    "\n",
    "vocab['cameron'] = vocab['<PAD>']\n",
    "vocab['<PAD>'] = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnwkQMSNEclC"
   },
   "outputs": [],
   "source": [
    "## delete\n",
    "del(token, tokens, i) \n",
    "del(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGlE0mMvCm5s"
   },
   "outputs": [],
   "source": [
    "# Creating the inverse dictionary of the vocab dictionary\n",
    "inv_vocab = {w:v for v, w in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIFbMl4WE-vD"
   },
   "outputs": [],
   "source": [
    "# Translating all the questions and the answers into integers \n",
    "# Replacing all the words that were filtered out by <OUT> \n",
    "\n",
    "encoder_inp = []\n",
    "for line in clean_question:\n",
    "    lst = []\n",
    "    for word in line.split():\n",
    "        if word not in vocab:\n",
    "            lst.append(vocab['<OUT>'])\n",
    "        else:\n",
    "            lst.append(vocab[word])\n",
    "        \n",
    "    encoder_inp.append(lst)\n",
    "\n",
    "decoder_inp = []\n",
    "for line in clean_answer:\n",
    "    lst = []\n",
    "    for word in line.split():\n",
    "        if word not in vocab:\n",
    "            lst.append(vocab['<OUT>'])\n",
    "        else:\n",
    "            lst.append(vocab[word])        \n",
    "    decoder_inp.append(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opZTU3XvFwCE"
   },
   "outputs": [],
   "source": [
    "### delete\n",
    "del(clean_answer, clean_question, line, lst, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qtATp9vGFw36"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_inp = pad_sequences(encoder_inp, 13, padding='post', truncating='post')\n",
    "decoder_inp = pad_sequences(decoder_inp, 13, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GF5Nie2GJdm"
   },
   "outputs": [],
   "source": [
    "\n",
    "decoder_output = []\n",
    "for i in decoder_inp:\n",
    "    decoder_output.append(i[1:]) \n",
    "\n",
    "decoder_output = pad_sequences(decoder_output, 13, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "#delete\n",
    "del(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6SPxprglHQTW",
    "outputId": "f57a04ca-0476-45d1-cf3e-2f5af81f2acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 13) (30000, 13) (30000, 13) 3005 3005 <PAD>\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "MAX_LEN = 13\n",
    "\n",
    "print(decoder_output.shape, decoder_inp.shape, encoder_inp.shape, len(vocab), len(inv_vocab), inv_vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tcwR5NrnHpe2",
    "outputId": "502e3fa2-847c-471b-a2cb-7e50b38aa27a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_vocab[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8qGorhtCIopx",
    "outputId": "adee1fbe-ca63-4ba7-9399-3d95b127fb54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 13, 3005)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to convert a class vector (integers) to binary class matrix.\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "decoder_output = to_categorical(decoder_output, len(vocab))\n",
    "\n",
    "\n",
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "mt-GVKwuI3Eh",
    "outputId": "9fee853c-c53f-4096-dabe-990156c4fad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Glove Embedding\n",
    "# algorithm for obtaining vector representations for words and is performed on aggregated global word-word co-occurrence statistics.\n",
    "\n",
    "#   !ls \"/content/gdrive/My Drive/Data/glove.6B.50d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('data/glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print(\"Glove Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIjtUu3eOVaz"
   },
   "outputs": [],
   "source": [
    "embedding_dimention = 50\n",
    "def embedding_matrix_creater(embedding_dimention, word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "embedding_matrix = embedding_matrix_creater(50, word_index=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEQo2X2OOZLX"
   },
   "outputs": [],
   "source": [
    "#delete\n",
    "del(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "3z3ZEQCXOs6q",
    "outputId": "66c29ca4-db2d-4617-fe56-6c15e243b31b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n",
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9kS_ffpDPCLo",
    "outputId": "b3a1a494-5d4b-4eb1-841a-6e4e7ded8724"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWdsTxOzOw8D"
   },
   "outputs": [],
   "source": [
    "embed = Embedding(VOCAB_SIZE+1, \n",
    "                  50,\n",
    "                  input_length=13,\n",
    "                  trainable=True)\n",
    "\n",
    "embed.build((None,))\n",
    "embed.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ksc0hzfMQtBY"
   },
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "enc_inp = Input(shape=(13, ))\n",
    "enc_embed = embed(enc_inp)\n",
    "enc_lstm = Bidirectional(LSTM(400, return_state=True, dropout=0.05, return_sequences = True))\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "enc_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "dec_inp = Input(shape=(13, ))\n",
    "dec_embed = embed(dec_inp)\n",
    "dec_lstm = LSTM(400*2, return_state=True, return_sequences=True, dropout=0.05)\n",
    "output, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n",
    "\n",
    "# attention\n",
    "attention = AttentionLayer()\n",
    "attention_op, attention_state = attention([encoder_outputs, output])\n",
    "decoder_concat_input = Concatenate(axis=-1)([output, attention_op])\n",
    "\n",
    "\n",
    "dec_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "output1 = dec_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([enc_inp, dec_inp], output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "rFNy-LpXRgsj",
    "outputId": "f6ef05c3-f678-44c0-f37c-2ce0e63fc742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 13, 50)       150300      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 13, 800), (N 1443200     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 800)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 800)          0           bidirectional[0][2]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 13, 800), (N 2723200     embedding[1][0]                  \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, 13, 800), (N 1280800     bidirectional[0][0]              \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 13, 1600)     0           lstm_1[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 13, 3005)     4811005     concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 10,408,505\n",
      "Trainable params: 10,408,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMZMR7XxR9_n"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nK5I2vNFVcA7",
    "outputId": "8806841a-ba76-4565-aa1c-d27ce9f2c9d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "797/797 [==============================] - 747s 937ms/step - loss: 2.9131 - accuracy: 0.5107 - val_loss: 2.6850 - val_accuracy: 0.5306\n",
      "Epoch 2/40\n",
      "797/797 [==============================] - 828s 1s/step - loss: 2.5523 - accuracy: 0.5409 - val_loss: 2.5744 - val_accuracy: 0.5414\n",
      "Epoch 3/40\n",
      "797/797 [==============================] - 747s 938ms/step - loss: 2.4012 - accuracy: 0.5496 - val_loss: 2.5478 - val_accuracy: 0.5455\n",
      "Epoch 4/40\n",
      "797/797 [==============================] - 744s 933ms/step - loss: 2.2647 - accuracy: 0.5569 - val_loss: 2.5604 - val_accuracy: 0.5465\n",
      "Epoch 5/40\n",
      "797/797 [==============================] - 744s 933ms/step - loss: 2.1225 - accuracy: 0.5654 - val_loss: 2.5898 - val_accuracy: 0.5483\n",
      "Epoch 6/40\n",
      "797/797 [==============================] - 748s 939ms/step - loss: 1.9730 - accuracy: 0.5794 - val_loss: 2.6397 - val_accuracy: 0.5470\n",
      "Epoch 7/40\n",
      "797/797 [==============================] - 749s 940ms/step - loss: 1.8194 - accuracy: 0.5971 - val_loss: 2.6941 - val_accuracy: 0.5450\n",
      "Epoch 8/40\n",
      "797/797 [==============================] - 788s 988ms/step - loss: 1.6676 - accuracy: 0.6195 - val_loss: 2.7570 - val_accuracy: 0.5414\n",
      "Epoch 9/40\n",
      "797/797 [==============================] - 849s 1s/step - loss: 1.5209 - accuracy: 0.6449 - val_loss: 2.8214 - val_accuracy: 0.5362\n",
      "Epoch 10/40\n",
      "797/797 [==============================] - 847s 1s/step - loss: 1.3841 - accuracy: 0.6721 - val_loss: 2.8968 - val_accuracy: 0.5328\n",
      "Epoch 11/40\n",
      "797/797 [==============================] - 793s 995ms/step - loss: 1.2561 - accuracy: 0.7002 - val_loss: 2.9688 - val_accuracy: 0.5309\n",
      "Epoch 12/40\n",
      "797/797 [==============================] - 744s 934ms/step - loss: 1.1438 - accuracy: 0.7260 - val_loss: 3.0398 - val_accuracy: 0.5292\n",
      "Epoch 13/40\n",
      "797/797 [==============================] - 768s 963ms/step - loss: 1.0409 - accuracy: 0.7511 - val_loss: 3.1152 - val_accuracy: 0.5263\n",
      "Epoch 14/40\n",
      "797/797 [==============================] - 754s 947ms/step - loss: 0.9560 - accuracy: 0.7708 - val_loss: 3.1968 - val_accuracy: 0.5226\n",
      "Epoch 15/40\n",
      "797/797 [==============================] - 745s 935ms/step - loss: 0.8789 - accuracy: 0.7904 - val_loss: 3.2707 - val_accuracy: 0.5220\n",
      "Epoch 16/40\n",
      "797/797 [==============================] - 778s 976ms/step - loss: 0.8152 - accuracy: 0.8055 - val_loss: 3.3400 - val_accuracy: 0.5195\n",
      "Epoch 17/40\n",
      "797/797 [==============================] - 738s 926ms/step - loss: 0.7666 - accuracy: 0.8172 - val_loss: 3.4134 - val_accuracy: 0.5175\n",
      "Epoch 18/40\n",
      "797/797 [==============================] - 735s 922ms/step - loss: 0.7142 - accuracy: 0.8303 - val_loss: 3.4818 - val_accuracy: 0.5183\n",
      "Epoch 19/40\n",
      "797/797 [==============================] - 734s 921ms/step - loss: 0.6731 - accuracy: 0.8406 - val_loss: 3.5489 - val_accuracy: 0.5162\n",
      "Epoch 20/40\n",
      "797/797 [==============================] - 737s 925ms/step - loss: 0.6402 - accuracy: 0.8472 - val_loss: 3.6014 - val_accuracy: 0.5165\n",
      "Epoch 21/40\n",
      "797/797 [==============================] - 734s 922ms/step - loss: 0.6109 - accuracy: 0.8548 - val_loss: 3.6706 - val_accuracy: 0.5148\n",
      "Epoch 22/40\n",
      "797/797 [==============================] - 735s 922ms/step - loss: 0.5885 - accuracy: 0.8593 - val_loss: 3.7165 - val_accuracy: 0.5146\n",
      "Epoch 23/40\n",
      "797/797 [==============================] - 733s 919ms/step - loss: 0.5671 - accuracy: 0.8640 - val_loss: 3.7644 - val_accuracy: 0.5134\n",
      "Epoch 24/40\n",
      "797/797 [==============================] - 734s 921ms/step - loss: 0.5474 - accuracy: 0.8681 - val_loss: 3.8160 - val_accuracy: 0.5121\n",
      "Epoch 25/40\n",
      "797/797 [==============================] - 734s 921ms/step - loss: 0.5320 - accuracy: 0.8714 - val_loss: 3.8648 - val_accuracy: 0.5139\n",
      "Epoch 26/40\n",
      "797/797 [==============================] - 735s 922ms/step - loss: 0.5213 - accuracy: 0.8736 - val_loss: 3.9009 - val_accuracy: 0.5119\n",
      "Epoch 27/40\n",
      "797/797 [==============================] - 737s 925ms/step - loss: 0.5063 - accuracy: 0.8773 - val_loss: 3.9543 - val_accuracy: 0.5123\n",
      "Epoch 28/40\n",
      "797/797 [==============================] - 733s 920ms/step - loss: 0.4943 - accuracy: 0.8797 - val_loss: 3.9857 - val_accuracy: 0.5101\n",
      "Epoch 29/40\n",
      "797/797 [==============================] - 735s 923ms/step - loss: 0.4866 - accuracy: 0.8811 - val_loss: 4.0173 - val_accuracy: 0.5113\n",
      "Epoch 30/40\n",
      "797/797 [==============================] - 735s 922ms/step - loss: 0.4783 - accuracy: 0.8827 - val_loss: 4.0535 - val_accuracy: 0.5098\n",
      "Epoch 31/40\n",
      "797/797 [==============================] - 736s 923ms/step - loss: 0.4742 - accuracy: 0.8830 - val_loss: 4.0823 - val_accuracy: 0.5110\n",
      "Epoch 32/40\n",
      "797/797 [==============================] - 737s 925ms/step - loss: 0.4642 - accuracy: 0.8853 - val_loss: 4.1126 - val_accuracy: 0.5121\n",
      "Epoch 33/40\n",
      "797/797 [==============================] - 736s 924ms/step - loss: 0.4604 - accuracy: 0.8858 - val_loss: 4.1437 - val_accuracy: 0.5117\n",
      "Epoch 34/40\n",
      "797/797 [==============================] - 739s 927ms/step - loss: 0.4541 - accuracy: 0.8870 - val_loss: 4.1636 - val_accuracy: 0.5101\n",
      "Epoch 35/40\n",
      "797/797 [==============================] - 739s 928ms/step - loss: 0.4500 - accuracy: 0.8877 - val_loss: 4.1872 - val_accuracy: 0.5098\n",
      "Epoch 36/40\n",
      "797/797 [==============================] - 733s 919ms/step - loss: 0.4434 - accuracy: 0.8894 - val_loss: 4.2098 - val_accuracy: 0.5104\n",
      "Epoch 37/40\n",
      "797/797 [==============================] - 738s 926ms/step - loss: 0.4414 - accuracy: 0.8893 - val_loss: 4.2387 - val_accuracy: 0.5072\n",
      "Epoch 38/40\n",
      "797/797 [==============================] - 732s 918ms/step - loss: 0.4386 - accuracy: 0.8894 - val_loss: 4.2644 - val_accuracy: 0.5077\n",
      "Epoch 39/40\n",
      "797/797 [==============================] - 734s 921ms/step - loss: 0.4359 - accuracy: 0.8904 - val_loss: 4.2833 - val_accuracy: 0.5086\n",
      "Epoch 40/40\n",
      "797/797 [==============================] - 733s 920ms/step - loss: 0.4302 - accuracy: 0.8918 - val_loss: 4.3072 - val_accuracy: 0.5088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28bd26bdf08>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_inp, decoder_inp], decoder_output, epochs=40, batch_size=32, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdvluAzZWLbb"
   },
   "outputs": [],
   "source": [
    "# Trained model that contains information about the model and has weights of the neurons\n",
    "model.save('chatbot.h5')\n",
    "model.save_weights('chatbot_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpTCbknXWmFT"
   },
   "outputs": [],
   "source": [
    "# Attention Interface\n",
    "\n",
    "encoder_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])\n",
    "\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "\n",
    "decoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)\n",
    "\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "#decoder_output = dec_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],\n",
    "                                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "colab_type": "code",
    "id": "TM2PBXJfX3dO",
    "outputId": "6da3456d-03a7-4d9b-c09a-e10a026352a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "#       Launch Chatbot          #\n",
      "==========================================\n",
      "you : hi\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "chatbot :  hi \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : how are you\n",
      "chatbot :  fine i am fine how are you \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : i am fine\n",
      "chatbot :  you sure \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : what are you doing\n",
      "chatbot :  i am <OUT> <OUT> \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : what is weather today\n",
      "chatbot :  i am going to take a big bath and order a \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : okay\n",
      "chatbot :  okay okay okay all right so long <OUT> \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : what is up\n",
      "chatbot :  i think you should give up <OUT> its dangerous \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : really\n",
      "chatbot :  yeah \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : I am happy to talk with you\n",
      "chatbot :  yeah \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : hmm\n",
      "chatbot :  i have been thinking \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : okay fine\n",
      "chatbot :  okay i gotta tell you i have discovered some things anyway \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : what\n",
      "chatbot :  i am not black \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : are you sure\n",
      "chatbot :  yeah i am sure \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : hmm\n",
      "chatbot :  i have been thinking \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : are you happy\n",
      "chatbot :  yeah but i am fine \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n",
      "you : goodbye\n",
      "chatbot :  goodbye \n",
      "::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "# Setting up the chat\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"#       Launch Chatbot          #\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "\n",
    "prepro1 = \"\"\n",
    "while prepro1 != 'q':\n",
    "    \n",
    "    prepro1 = input(\"you : \")\n",
    "    prepro = [prepro1]\n",
    "    \n",
    "    try:\n",
    "        txt = []\n",
    "        for x in prepro:\n",
    "            lst = []\n",
    "            for y in x.split():\n",
    "                lst.append(vocab[y])\n",
    "            txt.append(lst)\n",
    "        txt = pad_sequences(txt, 13, padding='post')\n",
    "\n",
    "\n",
    "        ###\n",
    "        enc_op, stat = encoder_model.predict( txt )\n",
    "\n",
    "        empty_target_seq = np.zeros( ( 1 , 1) )\n",
    "        empty_target_seq[0, 0] = vocab['<SOS>']\n",
    "        stop_condition = False\n",
    "        decoded_translation = ''\n",
    "\n",
    "\n",
    "        while not stop_condition :\n",
    "\n",
    "            dec_outputs , h , c = decoder_model.predict([ empty_target_seq ] + stat )\n",
    "\n",
    "            ###\n",
    "            ###########################\n",
    "            attention_op, attention_state = attention([enc_op, dec_outputs])\n",
    "            decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attention_op])\n",
    "            decoder_concat_input = dec_dense(decoder_concat_input)\n",
    "            ###########################\n",
    "\n",
    "            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
    "\n",
    "            sampled_word = inv_vocab[sampled_word_index] + ' '\n",
    "\n",
    "            if sampled_word != '<EOS> ':\n",
    "                decoded_translation += sampled_word           \n",
    "\n",
    "\n",
    "            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n",
    "                stop_condition = True\n",
    "\n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "            stat = [ h , c ] \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(\"chatbot : \", decoded_translation )\n",
    "    print(\"::::::::::::::::::::::::::::::::::::::::::::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
